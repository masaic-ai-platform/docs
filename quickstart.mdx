---
title: 'Quick Start Guide'
description: 'Get started with OpenResponses API in minutes'
---

# Quick Start Guide

This guide will help you get up and running with OpenResponses API quickly. We'll cover several deployment options and provide example API calls to get you started.

## Setup Options

<CardGroup cols={2}>
  <Card
    title="Basic Setup"
    icon="rocket"
    href="#basic-setup"
  >
    Run OpenResponses with default configuration
  </Card>
  <Card
    title="Tool-enabled Setup"
    icon="wrench"
    href="#tool-enabled-setup"
  >
    Run with built-in tools like web search and GitHub integration
  </Card>
  <Card
    title="Custom Tool Configuration"
    icon="gear"
    href="#custom-tool-configuration"
  >
    Configure your own custom tools
  </Card>
  <Card
    title="OpenAI Agent SDK Integration"
    icon="robot"
    href="#openai-agent-sdk-integration"
  >
    Run examples with OpenAI Agent SDK
  </Card>
  <Card
    title="Persistent Storage"
    icon="database"
    href="#setup-with-persistent-storage"
  >
    Configure with MongoDB for persistent storage
  </Card>
  <Card
    title="Observability Stack"
    icon="chart-line"
    href="#setup-with-observability"
  >
    Run with monitoring and observability capabilities
  </Card>
</CardGroup>

## Basic Setup

### Prerequisites

- Ensure port **8080** is available
- Docker daemon must be running on your local machine

### Run with Docker

The simplest way to get started is using Docker:

```bash
docker run -p 8080:8080 masaicai/open-responses:latest
```

### Clone the Repository

If you prefer using Docker Compose or want to explore the codebase, begin by cloning the repository:

```bash
git clone https://github.com/masaic-ai-platform/open-responses.git
cd open-responses
```

### Run with Docker Compose

```bash
# Start with Docker Compose
docker-compose up open-responses
```

### Example API Calls

Once OpenResponses is running, you can start making API calls. Here are some examples:

<Tabs>
  <Tab title="OpenAI">
    ```bash
    curl --location 'http://localhost:8080/v1/responses' \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer OPENAI_API_KEY' \
    --header 'x-model-provider: openai' \
    --data '{
        "model": "gpt-4o",
        "stream": false,
        "input": [
            {
                "role": "user",
                "content": "Tell me a joke"
            }
        ]
    }'
    ```
    Get your [OpenAI key here](https://platform.openai.com/settings)
  </Tab>
  <Tab title="Claude">
    ```bash
    curl --location 'http://localhost:8080/v1/responses' \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer ANTHROPIC_API_KEY' \
    --header 'x-model-provider: claude' \
    --data '{
        "model": "claude-3-5-sonnet-20241022",
        "stream": false,
        "input": [
            {
                "role": "user",
                "content": "Tell me a joke"
            }
        ]
    }'
    ```
    Get your [Anthropic key here](https://console.anthropic.com/dashboard)
  </Tab>
  <Tab title="Groq">
    ```bash
    curl --location 'http://localhost:8080/v1/responses' \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer GROQ_API_KEY' \
    --data '{
        "model": "llama-3.2-3b-preview",
        "stream": true,
        "input": [
            {
                "role": "user",
                "content": "Tell me a joke"
            }
        ]
    }'
    ```
    Open your [Groq key page](https://console.groq.com/keys) to create a key if you haven't done so already.
  </Tab>
</Tabs>

## Tool-enabled Setup

OpenResponses comes with built-in tools that enhance the capabilities of your AI applications. The pre-packaged `mcp-servers-config.json` file includes configurations for Brave Web Search and GitHub tools.

### Prerequisites

Before using built-in tools, you'll need:

- GitHub Personal Access Token - [Generate here](https://github.com/settings/personal-access-tokens)
- Brave Search API key - [Get from Brave API Dashboard](https://api-dashboard.search.brave.com/app/keys)

### Configuration

Create a `.env` file with your API keys:

```
GITHUB_TOKEN=your_github_token
BRAVE_API_KEY=your_brave_key_value
```

### Run with Built-in Tools

<Tabs>
  <Tab title="macOS">
    ```bash
    docker-compose --profile mcp up open-responses-mcp
    ```
  </Tab>
  <Tab title="Windows">
    ```bash
    docker-compose --profile mcp up open-responses-mcp-windows
    ```
  </Tab>
</Tabs>

<Note>
Stop previously running docker-compose (if any) before running this command.
</Note>

### Example Tool-enabled API Calls

<Tabs>
  <Tab title="Web Search">
    ```bash
    curl --location 'http://localhost:8080/v1/responses' \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer GROQ_API_KEY' \
    --data '{
        "model": "qwen-2.5-32b",
        "stream": false,
        "tools": [
            {
                "type": "brave_web_search"
            }
        ],
        "input": [
            {
                "role": "user",
                "content": "Where did NVIDIA GTC happened in 2025 and what were the major announcements?"
            }
        ]
    }'
    ```
  </Tab>
  <Tab title="GitHub Search">
    ```bash
    curl --location 'http://localhost:8080/v1/responses' \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer OPENAI_API_KEY' \
    --header 'x-model-provider: openai' \
    --data '{
        "model": "gpt-4o",
        "stream": false,
        "tools": [
            {
                "type": "search_repositories"
            }
        ],
        "input": [
            {
                "role": "user",
                "content": "Give me details of all repositories in github org masaic-ai-platform"
            }
        ]
    }'
    ```
  </Tab>
  <Tab title="Claude Think">
    ```bash
    curl --location 'http://localhost:8080/v1/responses' \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer ANTHROPIC_API_KEY' \
    --header 'x-model-provider: claude' \
    --data '{
        "model": "claude-3-7-sonnet-20250219",
        "stream": false,
        "tools": [
           {"type": "think"}
        ],
        "input": [
            {
                "role": "system",
                "content": "You are an experienced system design architect. Use the think tool to cross confirm thoughts before preparing the final answer."
            },
            {
                "role": "user",
                "content": "Give me the guidelines on designing a multi-agent distributed system with the following constraints in mind: 1. compute costs minimal, 2. the system should be horizontally scalable, 3. the behavior should be deterministic."
            }
        ]
    }'
    ```
  </Tab>
</Tabs>

## Custom Tool Configuration

If you want to add your own custom tools, you'll need to create or modify the `mcp-servers-config.json` file with your own tool definitions.

### Update the .env File

Add or update the following property in your `.env` file:

```
MCP_CONFIG_FILE_PATH=path_to_mcp_config_file
```

### Run the Service with Custom MCP Configuration

<Tabs>
  <Tab title="macOS">
    ```bash
    docker-compose --profile mcp up open-responses-custom-mcp
    ```
  </Tab>
  <Tab title="Windows">
    ```bash
    docker-compose --profile mcp up open-responses-custom-mcp-windows
    ```
  </Tab>
</Tabs>

<Note>
Stop previously running docker-compose (if any) before running this command.
</Note>

## OpenAI Agent SDK Integration

You can run examples provided by the openai-agent-python SDK using your locally deployed open-responses API.

### Running Example Scripts with OpenAI Agent SDK

1. Start the service using:

```bash
docker-compose up open-responses-with-openai
```

<Note>
Stop previously running docker-compose (if any) before running this command.
</Note>

2. Clone the forked repository:

```bash
git clone https://github.com/masaic-ai-platform/openai-agents-python.git
cd openai-agents-python
```

3. Configure the SDK in your Python script:

   - Define the environment variable `OPENAI_API_KEY` with your OpenAI API key
   - Define the environment variable `OPEN_RESPONSES_URL` to specify the URL for your local open-responses API
   - If not set, it will default to "http://localhost:8080/v1"

4. Run the examples:
   - Follow [Get started](https://github.com/masaic-ai-platform/openai-agents-python/tree/main?tab=readme-ov-file#get-started) instructions to install the SDK
   - All examples should work as expected except for the [research_bot example](https://github.com/masaic-ai-platform/openai-agents-python/tree/main/examples/research_bot) which uses OpenAI's proprietary WebSearchTool

### Running Agent Examples with Built-In Tools

Before running any example, ensure you are in the openai-agents-python folder.

1. Run the service with MCP tools:

<Tabs>
  <Tab title="macOS">
    ```bash
    docker-compose --profile mcp up open-responses-mcp
    ```
  </Tab>
  <Tab title="Windows">
    ```bash
    docker-compose --profile mcp up open-responses-mcp-windows
    ```
  </Tab>
</Tabs>

<Note>
  If you require SDK traces, ensure that you set the environment variable OPENAI_API_KEY.
  Otherwise, you may see a warning "OPENAI_API_KEY is not set, skipping trace export". 
  To disable tracing explicitly, add the statement:
  ```python
  set_tracing_disabled(disabled=False)
  ```
</Note>

2. Set up your environment variables:
   - `GROQ_API_KEY`
   - `OPEN_RESPONSES_URL`
   - `CLAUDE_API_KEY` (for specific examples)

3. Run the example scripts:

```bash
# Run agent_hands_off.py
python -m examples.open_responses.agent_hands_off
```

```bash
# Run brave_search_agent_with_groq.py
python -m examples.open_responses.brave_search_agent_with_groq
```

```bash
# Run brave_search_agent_with_groq_stream (requires CLAUDE_API_KEY)
python -m examples.open_responses.brave_search_agent_with_groq_stream
```

```bash
# Run think_tool_agent_with_claude.py (requires CLAUDE_API_KEY)
python -m examples.open_responses.think_tool_agent_with_claude
```

You can review the example scripts under the [open_responses examples](https://github.com/masaic-ai-platform/openai-agents-python/tree/main/examples/open_responses) repository.

## Setup with Persistent Storage

For production environments, you can enable persistent storage with MongoDB:

### Prerequisites
- Ensure port 8080 is available for the Open Responses service
- Ensure port 27017 is available for MongoDB
- Docker daemon must be running on your local machine

### Run with MongoDB

```bash
# Start MongoDB and OpenResponses
docker-compose --profile mongodb up
```

This will launch both the MongoDB database and the Open Responses service configured to use MongoDB as its storage backend.

### Testing Persistent Storage

To test that responses are being stored, make an API call with the `store` parameter set to `true`:

```bash
curl --location 'http://localhost:8080/v1/responses' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer OPENAI_API_KEY' \
--header 'x-model-provider: openai' \
--data '{
    "model": "gpt-4o",
    "store": true,
    "input": [
        {
            "role": "user",
            "content": "Write a short poem about persistence"
        }
    ]
}'
```

The response will include a `response_id` that you can use to reference this conversation in future requests.

### Following Up on a Stored Conversation

To continue a stored conversation, use the `previous_response_id` from your earlier response:

```bash
curl --location 'http://localhost:8080/v1/responses' \
--header 'Content-Type: application/json' \
--header 'Authorization: Bearer OPENAI_API_KEY' \
--header 'x-model-provider: openai' \
--data '{
    "model": "gpt-4o",
    "store": true,
    "previous_response_id": "resp_abc123",
    "input": [
        {
            "role": "user",
            "content": "Make it longer and add a rhyme scheme"
        }
    ]
}'
```

For more details on response storage options and configuration, refer to the [Response Store Configuration](/openresponses/store) documentation.

## Setup with Observability

OpenResponses can be run with a comprehensive observability stack that includes OpenTelemetry, Jaeger, Prometheus, and Grafana. This setup provides detailed monitoring, tracing, and metrics visualization capabilities.

For detailed instructions on setting up and running OpenResponses with the observability stack, please refer to our [Running with Observability Stack](/openresponses/runwithobs) guide.

## Next Steps

- Explore [OpenAI Compatibility](/openresponses/compatibility) for detailed API compatibility information
- Learn about [Tool Integrations](/api-reference/endpoint/tools) for extending functionality
- Configure [Observability](/openresponses/observability) for production monitoring
